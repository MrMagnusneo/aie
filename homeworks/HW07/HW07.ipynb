{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585affaa-9e36-41e5-a483-b382da199a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Загрузка всех трех датасетов\n",
    "df1 = pd.read_csv('data/S07-hw-dataset-01.csv')\n",
    "df2 = pd.read_csv('data/S07-hw-dataset-02.csv')\n",
    "df3 = pd.read_csv('data/S07-hw-dataset-03.csv', header=None)\n",
    "df3.columns = ['sample_id', 'feature1', 'feature2', 'feature3', 'feature4']\n",
    "\n",
    "# Сохранение sample_id для каждого датасета\n",
    "sample_ids_ds1 = df1['sample_id']\n",
    "sample_ids_ds2 = df2['sample_id']\n",
    "sample_ids_ds3 = df3['sample_id']\n",
    "\n",
    "# Удаление sample_id из признаков\n",
    "X_ds1 = df1.drop(columns=['sample_id'])\n",
    "X_ds2 = df2.drop(columns=['sample_id'])\n",
    "X_ds3 = df3.drop(columns=['sample_id'])\n",
    "\n",
    "# Функция для вывода первичного анализа\n",
    "def initial_analysis(df, dataset_name):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Анализ {dataset_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Размер датасета: {df.shape}\")\n",
    "    print(\"\\nПервые 5 строк:\")\n",
    "    display(df.head())\n",
    "    print(\"\\nИнформация о данных:\")\n",
    "    display(df.info())\n",
    "    print(\"\\nБазовые статистики:\")\n",
    "    display(df.describe())\n",
    "    print(\"\\nКоличество пропусков:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Определение типов признаков\n",
    "    numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nЧисловые признаки: {numerical_features}\")\n",
    "    print(f\"Категориальные признаки: {categorical_features}\")\n",
    "\n",
    "# Проведение первичного анализа для каждого датасета\n",
    "initial_analysis(df1, \"Dataset 01\")\n",
    "initial_analysis(df2, \"Dataset 02\")\n",
    "initial_analysis(df3, \"Dataset 03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569006c1-5645-4282-832b-cc45ba5ca9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Загрузка данных\n",
    "# Dataset 1 и 2 загружаем как обычно\n",
    "df1 = pd.read_csv('data/S07-hw-dataset-01.csv')\n",
    "df2 = pd.read_csv('data/S07-hw-dataset-02.csv')\n",
    "\n",
    "# Костыль\n",
    "try:\n",
    "    df3 = pd.read_csv('data/S07-hw-dataset-03.csv')\n",
    "    \n",
    "    if 'x1' in df3.columns or (df3.iloc[:, 1].dtype == 'object' and df3.iloc[0, 1] == 'x1'):\n",
    "        df3 = pd.read_csv('data/S07-hw-dataset-03.csv', header=None)\n",
    "        df3.columns = ['sample_id', 'feature1', 'feature2', 'feature3', 'feature4']\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при загрузке Dataset 03: {e}\")\n",
    "    print(\"Пытаемся загрузить без заголовков...\")\n",
    "    df3 = pd.read_csv('data/S07-hw-dataset-03.csv', header=None)\n",
    "    if df3.shape[1] == 5:\n",
    "        df3.columns = ['sample_id', 'feature1', 'feature2', 'feature3', 'feature4']\n",
    "    elif df3.shape[1] == 4:\n",
    "        df3.columns = ['feature1', 'feature2', 'feature3', 'feature4']\n",
    "        df3.insert(0, 'sample_id', range(len(df3)))\n",
    "    else:\n",
    "        print(f\"Неожиданное количество столбцов: {df3.shape[1]}\")\n",
    "        df3.columns = ['sample_id', 'feature1', 'feature2', 'feature3', 'feature4'] + \\\n",
    "                     [f'feature{i}' for i in range(5, df3.shape[1]+1)]\n",
    "\n",
    "sample_ids_ds1 = df1['sample_id']\n",
    "sample_ids_ds2 = df2['sample_id']\n",
    "sample_ids_ds3 = df3['sample_id']\n",
    "\n",
    "X_ds1 = df1.drop(columns=['sample_id'])\n",
    "X_ds2 = df2.drop(columns=['sample_id'])\n",
    "X_ds3 = df3.drop(columns=['sample_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f7fcd-66c6-4267-a0ed-d1591ea03318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    \"\"\"Препроцессинг данных с обработкой разных типов данных\"\"\"\n",
    "    # Создаем копию данных для безопасности\n",
    "    X_processed = X.copy()\n",
    "    \n",
    "    # Преобразуем все столбцы в числовой формат, если возможно\n",
    "    for col in X_processed.columns:\n",
    "        # Проверяем, можно ли преобразовать столбец в числовой формат\n",
    "        try:\n",
    "            # Попытка преобразовать в числовой формат\n",
    "            X_processed[col] = pd.to_numeric(X_processed[col], errors='coerce')\n",
    "        except:\n",
    "            # Если не удалось, помечаем столбец для особой обработки\n",
    "            print(f\"Предупреждение: столбец {col} содержит нечисловые значения\")\n",
    "    \n",
    "    # Проверяем наличие категориальных признаков\n",
    "    categorical_features = X_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    if categorical_features:\n",
    "        print(f\"Обнаружены категориальные признаки: {categorical_features}\")\n",
    "        # Преобразуем категориальные признаки в dummy-переменные\n",
    "        X_processed = pd.get_dummies(X_processed, columns=categorical_features, drop_first=True)\n",
    "    \n",
    "    # Обработка пропусков\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_imputed = imputer.fit_transform(X_processed)\n",
    "    \n",
    "    # Масштабирование\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_imputed)\n",
    "    \n",
    "    return X_scaled\n",
    "\n",
    "# Применение препроцессинга к каждому датасету\n",
    "print(\"\\nПрепроцессинг Dataset 01...\")\n",
    "X_processed_ds1 = preprocess_data(X_ds1)\n",
    "\n",
    "print(\"\\nПрепроцессинг Dataset 02...\")\n",
    "X_processed_ds2 = preprocess_data(X_ds2)\n",
    "\n",
    "print(\"\\nПрепроцессинг Dataset 03...\")\n",
    "# Для Dataset 03 дополнительно проверяем структуру данных перед препроцессингом\n",
    "print(\"Структура Dataset 03 перед препроцессингом:\")\n",
    "print(f\"Размер: {X_ds3.shape}\")\n",
    "print(\"Типы данных:\")\n",
    "print(X_ds3.dtypes)\n",
    "print(\"Первые 2 строки:\")\n",
    "print(X_ds3.head(2))\n",
    "\n",
    "X_processed_ds3 = preprocess_data(X_ds3)\n",
    "\n",
    "print(\"\\nПрепроцессинг успешно завершен для всех датасетов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a460c8-ea85-4d71-b30c-d172c2972656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "def run_clustering_algorithms(X, dataset_name, max_clusters=15):\n",
    "    \"\"\"Запуск и оценка алгоритмов кластеризации для одного датасета\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # KMeans с подбором оптимального k\n",
    "    print(f\"\\nЗапуск KMeans для {dataset_name}...\")\n",
    "    silhouette_scores = []\n",
    "    db_scores = []\n",
    "    ch_scores = []\n",
    "    \n",
    "    for k in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        silhouette_scores.append(silhouette_score(X, labels))\n",
    "        db_scores.append(davies_bouldin_score(X, labels))\n",
    "        ch_scores.append(calinski_harabasz_score(X, labels))\n",
    "    \n",
    "    # Выбор оптимального k по silhouette score\n",
    "    best_k = range(2, max_clusters + 1)[np.argmax(silhouette_scores)]\n",
    "    best_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "    best_kmeans_labels = best_kmeans.fit_predict(X)\n",
    "    \n",
    "    # Сохранение результатов KMeans\n",
    "    results['kmeans'] = {\n",
    "        'best_k': best_k,\n",
    "        'labels': best_kmeans_labels,\n",
    "        'silhouette': silhouette_scores,\n",
    "        'db_score': db_scores,\n",
    "        'ch_score': ch_scores,\n",
    "        'best_metrics': {\n",
    "            'silhouette': silhouette_score(X, best_kmeans_labels),\n",
    "            'db_score': davies_bouldin_score(X, best_kmeans_labels),\n",
    "            'ch_score': calinski_harabasz_score(X, best_kmeans_labels)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # DBSCAN с подбором параметров\n",
    "    print(f\"Запуск DBSCAN для {dataset_name}...\")\n",
    "    eps_range = np.linspace(0.1, 3.0, 30)\n",
    "    min_samples = 5\n",
    "    dbscan_results = []\n",
    "    \n",
    "    for eps in eps_range:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(X)\n",
    "        \n",
    "        # Доля шума\n",
    "        noise_ratio = np.sum(labels == -1) / len(labels)\n",
    "        \n",
    "        # Оценка метрик только на не-шумовых точках\n",
    "        non_noise_mask = labels != -1\n",
    "        if np.sum(non_noise_mask) > 1 and len(np.unique(labels[non_noise_mask])) > 1:\n",
    "            silhouette = silhouette_score(X[non_noise_mask], labels[non_noise_mask])\n",
    "            db = davies_bouldin_score(X[non_noise_mask], labels[non_noise_mask])\n",
    "            ch = calinski_harabasz_score(X[non_noise_mask], labels[non_noise_mask])\n",
    "        else:\n",
    "            silhouette = db = ch = np.nan\n",
    "        \n",
    "        dbscan_results.append({\n",
    "            'eps': eps,\n",
    "            'min_samples': min_samples,\n",
    "            'noise_ratio': noise_ratio,\n",
    "            'silhouette': silhouette,\n",
    "            'db_score': db,\n",
    "            'ch_score': ch,\n",
    "            'n_clusters': len(np.unique(labels[non_noise_mask])) if np.sum(non_noise_mask) > 0 else 0\n",
    "        })\n",
    "    \n",
    "    # Выбор лучших параметров DBSCAN\n",
    "    valid_results = [r for r in dbscan_results if not np.isnan(r['silhouette']) and r['silhouette'] > 0]\n",
    "    if valid_results:\n",
    "        best_dbscan = max(valid_results, key=lambda x: x['silhouette'])\n",
    "        best_dbscan_model = DBSCAN(eps=best_dbscan['eps'], min_samples=best_dbscan['min_samples'])\n",
    "        best_dbscan_labels = best_dbscan_model.fit_predict(X)\n",
    "        \n",
    "        results['dbscan'] = {\n",
    "            'best_eps': best_dbscan['eps'],\n",
    "            'min_samples': best_dbscan['min_samples'],\n",
    "            'noise_ratio': best_dbscan['noise_ratio'],\n",
    "            'n_clusters': best_dbscan['n_clusters'],\n",
    "            'labels': best_dbscan_labels,\n",
    "            'results': dbscan_results,\n",
    "            'best_metrics': {\n",
    "                'silhouette': best_dbscan['silhouette'],\n",
    "                'db_score': best_dbscan['db_score'],\n",
    "                'ch_score': best_dbscan['ch_score']\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        results['dbscan'] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Запуск алгоритмов для всех датасетов\n",
    "results_ds1 = run_clustering_algorithms(X_processed_ds1, 'Dataset 01')\n",
    "results_ds2 = run_clustering_algorithms(X_processed_ds2, 'Dataset 02')\n",
    "results_ds3 = run_clustering_algorithms(X_processed_ds3, 'Dataset 03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1302000e-23e0-4394-82ee-126f8db9bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_pca_clusters(X, labels, title, filename):\n",
    "    \"\"\"Визуализация кластеров с помощью PCA\"\"\"\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(\n",
    "        X_pca[:, 0], \n",
    "        X_pca[:, 1], \n",
    "        c=labels, \n",
    "        cmap='viridis', \n",
    "        alpha=0.6,\n",
    "        s=30,\n",
    "        edgecolors='w',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    \n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(f'PCA 1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
    "    plt.ylabel(f'PCA 2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.savefig(f'artifacts/figures/{filename}', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_vs_param(param_values, metric_values, param_name, metric_name, title, filename):\n",
    "    \"\"\"Построение графика метрик в зависимости от параметра\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(param_values, metric_values, 'b-o', linewidth=2, markersize=6)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(param_name, fontsize=12)\n",
    "    plt.ylabel(metric_name, fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.savefig(f'artifacts/figures/{filename}', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Визуализация PCA для каждого датасета с лучшим методом\n",
    "plot_pca_clusters(X_processed_ds1, results_ds1['kmeans']['labels'], \n",
    "                  f'PCA: KMeans Clusters (k={results_ds1[\"kmeans\"][\"best_k\"]}) - Dataset 01', \n",
    "                  'pca_clusters_ds1.png')\n",
    "\n",
    "plot_pca_clusters(X_processed_ds2, results_ds2['kmeans']['labels'], \n",
    "                  f'PCA: KMeans Clusters (k={results_ds2[\"kmeans\"][\"best_k\"]}) - Dataset 02', \n",
    "                  'pca_clusters_ds2.png')\n",
    "\n",
    "plot_pca_clusters(X_processed_ds3, results_ds3['kmeans']['labels'], \n",
    "                  f'PCA: KMeans Clusters (k={results_ds3[\"kmeans\"][\"best_k\"]}) - Dataset 03', \n",
    "                  'pca_clusters_ds3.png')\n",
    "\n",
    "# Визуализация метрик для KMeans (silhouette vs k)\n",
    "plot_metrics_vs_param(\n",
    "    range(2, 16),\n",
    "    results_ds1['kmeans']['silhouette'],\n",
    "    'Number of clusters (k)',\n",
    "    'Silhouette Score',\n",
    "    'Silhouette Score vs k - Dataset 01',\n",
    "    'silhouette_vs_k_ds1.png'\n",
    ")\n",
    "\n",
    "plot_metrics_vs_param(\n",
    "    range(2, 16),\n",
    "    results_ds2['kmeans']['silhouette'],\n",
    "    'Number of clusters (k)',\n",
    "    'Silhouette Score',\n",
    "    'Silhouette Score vs k - Dataset 02',\n",
    "    'silhouette_vs_k_ds2.png'\n",
    ")\n",
    "\n",
    "plot_metrics_vs_param(\n",
    "    range(2, 16),\n",
    "    results_ds3['kmeans']['silhouette'],\n",
    "    'Number of clusters (k)',\n",
    "    'Silhouette Score',\n",
    "    'Silhouette Score vs k - Dataset 03',\n",
    "    'silhouette_vs_k_ds3.png'\n",
    ")\n",
    "\n",
    "# Визуализация метрик для DBSCAN (silhouette vs eps)\n",
    "if results_ds1['dbscan'] is not None:\n",
    "    plot_metrics_vs_param(\n",
    "        [r['eps'] for r in results_ds1['dbscan']['results']],\n",
    "        [r['silhouette'] for r in results_ds1['dbscan']['results']],\n",
    "        'eps',\n",
    "        'Silhouette Score',\n",
    "        'Silhouette Score vs eps - Dataset 01',\n",
    "        'silhouette_vs_eps_ds1.png'\n",
    "    )\n",
    "\n",
    "if results_ds2['dbscan'] is not None:\n",
    "    plot_metrics_vs_param(\n",
    "        [r['eps'] for r in results_ds2['dbscan']['results']],\n",
    "        [r['silhouette'] for r in results_ds2['dbscan']['results']],\n",
    "        'eps',\n",
    "        'Silhouette Score',\n",
    "        'Silhouette Score vs eps - Dataset 02',\n",
    "        'silhouette_vs_eps_ds2.png'\n",
    "    )\n",
    "\n",
    "if results_ds3['dbscan'] is not None:\n",
    "    plot_metrics_vs_param(\n",
    "        [r['eps'] for r in results_ds3['dbscan']['results']],\n",
    "        [r['silhouette'] for r in results_ds3['dbscan']['results']],\n",
    "        'eps',\n",
    "        'Silhouette Score',\n",
    "        'Silhouette Score vs eps - Dataset 03',\n",
    "        'silhouette_vs_eps_ds3.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb932d-c5c2-46ef-bacf-bd91056b81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "def check_stability(X, n_clusters=4, n_runs=5, dataset_name='Dataset'):\n",
    "    \"\"\"Проверка устойчивости KMeans\"\"\"\n",
    "    ari_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=i, n_init=10)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "        if i > 0:\n",
    "            ari = adjusted_rand_score(all_labels[i-1], labels)\n",
    "            ari_scores.append(ari)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, n_runs), ari_scores, 'bo-', linewidth=2, markersize=8)\n",
    "    plt.title(f'Устойчивость KMeans (k={n_clusters}) - {dataset_name}', fontsize=14)\n",
    "    plt.xlabel('Номер запуска', fontsize=12)\n",
    "    plt.ylabel('Adjusted Rand Index', fontsize=12)\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.savefig(f'artifacts/figures/stability_{dataset_name.lower().replace(\" \", \"_\")}.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return np.mean(ari_scores), np.std(ari_scores)\n",
    "\n",
    "# Проверка устойчивости для Dataset 02\n",
    "print(\"\\nПроверка устойчивости для Dataset 02...\")\n",
    "mean_ari, std_ari = check_stability(\n",
    "    X_processed_ds2, \n",
    "    n_clusters=results_ds2['kmeans']['best_k'], \n",
    "    dataset_name='Dataset 02'\n",
    ")\n",
    "print(f\"Средний ARI: {mean_ari:.4f}, Стандартное отклонение: {std_ari:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134a2ef-494f-487a-a33b-601038373713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def select_best_algorithm(results, dataset_name):\n",
    "    \"\"\"Выбор лучшего алгоритма для датасета\"\"\"\n",
    "    kmeans_score = results['kmeans']['best_metrics']['silhouette']\n",
    "    dbscan_score = results['dbscan']['best_metrics']['silhouette'] if results['dbscan'] else -1\n",
    "    \n",
    "    # Выбираем KMeans, если он лучше или если DBSCAN не применим\n",
    "    if dbscan_score > kmeans_score and results['dbscan']['noise_ratio'] < 0.3:\n",
    "        return 'dbscan'\n",
    "    else:\n",
    "        return 'kmeans'\n",
    "\n",
    "# Определение лучших алгоритмов для каждого датасета\n",
    "best_alg_ds1 = select_best_algorithm(results_ds1, 'Dataset 01')\n",
    "best_alg_ds2 = select_best_algorithm(results_ds2, 'Dataset 02')\n",
    "best_alg_ds3 = select_best_algorithm(results_ds3, 'Dataset 03')\n",
    "\n",
    "print(f\"\\nЛучший алгоритм для Dataset 01: {best_alg_ds1}\")\n",
    "print(f\"Лучший алгоритм для Dataset 02: {best_alg_ds2}\")\n",
    "print(f\"Лучший алгоритм для Dataset 03: {best_alg_ds3}\")\n",
    "\n",
    "# Подготовка и сохранение metrics_summary.json\n",
    "metrics_summary = {\n",
    "    \"dataset_01\": {\n",
    "        \"kmeans\": {\n",
    "            \"silhouette\": results_ds1['kmeans']['best_metrics']['silhouette'],\n",
    "            \"davies_bouldin\": results_ds1['kmeans']['best_metrics']['db_score'],\n",
    "            \"calinski_harabasz\": results_ds1['kmeans']['best_metrics']['ch_score']\n",
    "        },\n",
    "        \"dbscan\": {\n",
    "            \"silhouette\": results_ds1['dbscan']['best_metrics']['silhouette'] if results_ds1['dbscan'] else None,\n",
    "            \"davies_bouldin\": results_ds1['dbscan']['best_metrics']['db_score'] if results_ds1['dbscan'] else None,\n",
    "            \"calinski_harabasz\": results_ds1['dbscan']['best_metrics']['ch_score'] if results_ds1['dbscan'] else None,\n",
    "            \"noise_ratio\": results_ds1['dbscan']['noise_ratio'] if results_ds1['dbscan'] else None\n",
    "        }\n",
    "    },\n",
    "    \"dataset_02\": {\n",
    "        \"kmeans\": {\n",
    "            \"silhouette\": results_ds2['kmeans']['best_metrics']['silhouette'],\n",
    "            \"davies_bouldin\": results_ds2['kmeans']['best_metrics']['db_score'],\n",
    "            \"calinski_harabasz\": results_ds2['kmeans']['best_metrics']['ch_score']\n",
    "        },\n",
    "        \"dbscan\": {\n",
    "            \"silhouette\": results_ds2['dbscan']['best_metrics']['silhouette'] if results_ds2['dbscan'] else None,\n",
    "            \"davies_bouldin\": results_ds2['dbscan']['best_metrics']['db_score'] if results_ds2['dbscan'] else None,\n",
    "            \"calinski_harabasz\": results_ds2['dbscan']['best_metrics']['ch_score'] if results_ds2['dbscan'] else None,\n",
    "            \"noise_ratio\": results_ds2['dbscan']['noise_ratio'] if results_ds2['dbscan'] else None\n",
    "        }\n",
    "    },\n",
    "    \"dataset_03\": {\n",
    "        \"kmeans\": {\n",
    "            \"silhouette\": results_ds3['kmeans']['best_metrics']['silhouette'],\n",
    "            \"davies_bouldin\": results_ds3['kmeans']['best_metrics']['db_score'],\n",
    "            \"calinski_harabasz\": results_ds3['kmeans']['best_metrics']['ch_score']\n",
    "        },\n",
    "        \"dbscan\": {\n",
    "            \"silhouette\": results_ds3['dbscan']['best_metrics']['silhouette'] if results_ds3['dbscan'] else None,\n",
    "            \"davies_bouldin\": results_ds3['dbscan']['best_metrics']['db_score'] if results_ds3['dbscan'] else None,\n",
    "            \"calinski_harabasz\": results_ds3['dbscan']['best_metrics']['ch_score'] if results_ds3['dbscan'] else None,\n",
    "            \"noise_ratio\": results_ds3['dbscan']['noise_ratio'] if results_ds3['dbscan'] else None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('artifacts/metrics_summary.json', 'w') as f:\n",
    "    json.dump(metrics_summary, f, indent=4)\n",
    "\n",
    "# Подготовка и сохранение best_configs.json\n",
    "best_configs = {\n",
    "    \"dataset_01\": {\n",
    "        \"best_algorithm\": best_alg_ds1,\n",
    "        \"parameters\": {\n",
    "            \"k\": results_ds1['kmeans']['best_k'] if best_alg_ds1 == 'kmeans' else None,\n",
    "            \"eps\": results_ds1['dbscan']['best_eps'] if best_alg_ds1 == 'dbscan' else None,\n",
    "            \"min_samples\": results_ds1['dbscan']['min_samples'] if best_alg_ds1 == 'dbscan' else None\n",
    "        },\n",
    "        \"selection_criterion\": \"silhouette_score\"\n",
    "    },\n",
    "    \"dataset_02\": {\n",
    "        \"best_algorithm\": best_alg_ds2,\n",
    "        \"parameters\": {\n",
    "            \"k\": results_ds2['kmeans']['best_k'] if best_alg_ds2 == 'kmeans' else None,\n",
    "            \"eps\": results_ds2['dbscan']['best_eps'] if best_alg_ds2 == 'dbscan' else None,\n",
    "            \"min_samples\": results_ds2['dbscan']['min_samples'] if best_alg_ds2 == 'dbscan' else None\n",
    "        },\n",
    "        \"selection_criterion\": \"silhouette_score\"\n",
    "    },\n",
    "    \"dataset_03\": {\n",
    "        \"best_algorithm\": best_alg_ds3,\n",
    "        \"parameters\": {\n",
    "            \"k\": results_ds3['kmeans']['best_k'] if best_alg_ds3 == 'kmeans' else None,\n",
    "            \"eps\": results_ds3['dbscan']['best_eps'] if best_alg_ds3 == 'dbscan' else None,\n",
    "            \"min_samples\": results_ds3['dbscan']['min_samples'] if best_alg_ds3 == 'dbscan' else None\n",
    "        },\n",
    "        \"selection_criterion\": \"silhouette_score\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('artifacts/best_configs.json', 'w') as f:\n",
    "    json.dump(best_configs, f, indent=4)\n",
    "\n",
    "# Сохранение меток кластеров для лучших моделей\n",
    "best_labels_ds1 = results_ds1[best_alg_ds1]['labels']\n",
    "best_labels_df1 = pd.DataFrame({\n",
    "    'sample_id': sample_ids_ds1,\n",
    "    'cluster_label': best_labels_ds1\n",
    "})\n",
    "best_labels_df1.to_csv('artifacts/labels/labels_hw07_ds1.csv', index=False)\n",
    "\n",
    "best_labels_ds2 = results_ds2[best_alg_ds2]['labels']\n",
    "best_labels_df2 = pd.DataFrame({\n",
    "    'sample_id': sample_ids_ds2,\n",
    "    'cluster_label': best_labels_ds2\n",
    "})\n",
    "best_labels_df2.to_csv('artifacts/labels/labels_hw07_ds2.csv', index=False)\n",
    "\n",
    "best_labels_ds3 = results_ds3[best_alg_ds3]['labels']\n",
    "best_labels_df3 = pd.DataFrame({\n",
    "    'sample_id': sample_ids_ds3,\n",
    "    'cluster_label': best_labels_ds3\n",
    "})\n",
    "best_labels_df3.to_csv('artifacts/labels/labels_hw07_ds3.csv', index=False)\n",
    "\n",
    "print(\"\\nВсе артефакты успешно сохранены:\")\n",
    "print(\"- metrics_summary.json\")\n",
    "print(\"- best_configs.json\")\n",
    "print(\"- labels_hw07_ds1.csv, labels_hw07_ds2.csv, labels_hw07_ds3.csv\")\n",
    "print(\"- 6 графиков в artifacts/figures/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
